{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset definition"
      ],
      "metadata": {
        "id": "Og5rxc1Xzrqm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlc_Dpa917O8",
        "outputId": "86293c94-c6a6-4621-b667-46c7b32b006a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is available!  Training on GPU ...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Colab Notebooks/Image and Video recognition/dataset\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "  print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "  print('CUDA is available!  Training on GPU ...')\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab Notebooks/Image and Video recognition/dataset\n",
        "root = \"/content/drive/MyDrive/Colab Notebooks/Image and Video recognition/dataset/\"\n",
        "# root = \"dataset/\"\n",
        "\n",
        "class CustomDatasetFromCSV(Dataset):\n",
        "    def __init__(self, csv_path, transform = None):\n",
        "        self.data = pd.read_csv(root + csv_path)\n",
        "        self.contents = np.asarray(self.data['content'])\n",
        "        self.fontencoder = LabelEncoder()\n",
        "        self.fonts = self.fontencoder.fit_transform(np.asarray(self.data['font']))\n",
        "        self.authors = np.asarray(self.data['author'])\n",
        "        self.num_data = len(self.contents)\n",
        "        self.num_classes = len(self.fontencoder.classes_)\n",
        "        self.images = np.asarray([ np.array(Image.open(root + self.data['word_path'][i])) / 255. for i in tqdm(range(self.num_data)) ])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        content = self.contents[index]\n",
        "        font = self.fonts[index]\n",
        "        author = self.authors[index]\n",
        "        return image, content, font, author\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "A9A5p8iAWDiJ"
      },
      "outputs": [],
      "source": [
        "def split_dataset(dataset, batch_size, split_size, method = \"all\", shuffle_dataset = True):\n",
        "  if method == \"all\":\n",
        "    indices = list(range(len(dataset)))\n",
        "    if shuffle_dataset :\n",
        "        np.random.seed(0)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    split_1 = int(np.floor(split_size[0] * len(dataset)))\n",
        "    split_2 = int(np.floor((split_size[0] + split_size[1]) * len(dataset)))\n",
        "    train_indices, val_indices, test_indices = indices[:split_1], indices[split_1:split_2], indices[split_2:]\n",
        "  else:\n",
        "    print(\"wrong method!!\")\n",
        "\n",
        "  # Creating PT data samplers and loaders:\n",
        "  train_sampler = SubsetRandomSampler(train_indices)\n",
        "  valid_sampler = SubsetRandomSampler(val_indices)\n",
        "  test_sampler = SubsetRandomSampler(test_indices)\n",
        "  train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "  validation_loader = DataLoader(dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "  test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler)\n",
        "\n",
        "  return train_loader, validation_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet model definition\n",
        "https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py"
      ],
      "metadata": {
        "id": "eXAWTOOUzaaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion: int = 1\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion: int = 4\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes: int,\n",
        "        planes: int,\n",
        "        stride: int = 1,\n",
        "        downsample: Optional[nn.Module] = None,\n",
        "        groups: int = 1,\n",
        "        base_width: int = 64,\n",
        "        dilation: int = 1,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        layers: List[int],\n",
        "        num_classes: int = 1000,\n",
        "        zero_init_residual: bool = False,\n",
        "        groups: int = 1,\n",
        "        width_per_group: int = 64,\n",
        "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
        "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # _log_api_usage_once(self)\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: Type[Union[BasicBlock, Bottleneck]],\n",
        "        planes: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1,\n",
        "        dilate: bool = False,\n",
        "    ) -> nn.Sequential:\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
        "        # See note [TorchScript super()]\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self._forward_impl(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "1xPUCEgLHvnZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SE-ResNet, SE-ResNeXt model definition  \n",
        "https://github.com/StickCui/PyTorch-SE-ResNet"
      ],
      "metadata": {
        "id": "Do_LHZuv5C1N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Selayer(nn.Module):\n",
        "\n",
        "    def __init__(self, inplanes):\n",
        "        super(Selayer, self).__init__()\n",
        "        self.global_avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv1 = nn.Conv2d(inplanes, inplanes / 16, kernel_size=1, stride=1)\n",
        "        self.conv2 = nn.Conv2d(inplanes / 16, inplanes, kernel_size=1, stride=1)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.global_avgpool(x)\n",
        "\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        return x * out\n",
        "\n",
        "class SEBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(SEBottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        # SE\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv_down = nn.Conv2d(\n",
        "            planes * 4, planes // 4, kernel_size=1, bias=False)\n",
        "        self.conv_up = nn.Conv2d(\n",
        "            planes // 4, planes * 4, kernel_size=1, bias=False)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        # Downsample\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out1 = self.global_pool(out)\n",
        "        out1 = self.conv_down(out1)\n",
        "        out1 = self.relu(out1)\n",
        "        out1 = self.conv_up(out1)\n",
        "        out1 = self.sig(out1)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        res = out1 * out + residual\n",
        "        res = self.relu(res)\n",
        "\n",
        "        return res\n",
        "\n",
        "class BottleneckX(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, cardinality, stride=1, downsample=None):\n",
        "        super(BottleneckX, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(planes * 2, planes * 2, kernel_size=3, stride=stride,\n",
        "                               padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes * 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(planes * 2, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "\n",
        "        self.selayer = Selayer(planes * 4)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        out = self.selayer(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000):\n",
        "        self.inplanes = 64\n",
        "        super(SEResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class SEResNeXt(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, cardinality=32, num_classes=1000):\n",
        "        super(SEResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.inplanes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, self.cardinality, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, self.cardinality))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kufOCjVN2tSP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiated dataset, model and optimizer"
      ],
      "metadata": {
        "id": "P3x4xx3L5jxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToVE8nB9RvBd",
        "outputId": "7cc6484b-ad5f-476f-bcae-a0b7576d9459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2896/2896 [00:03<00:00, 803.96it/s]\n"
          ]
        }
      ],
      "source": [
        "annotated_file = \"annotated_merged_deleted_x_2.csv\"\n",
        "dataset = CustomDatasetFromCSV(annotated_file)\n",
        "\n",
        "batch_size = 32\n",
        "split_size = [0.8, 0.1, 0.1]\n",
        "train_loader, valid_loader, test_loader = split_dataset(dataset, batch_size, split_size, method = \"all\", shuffle_dataset = True)\n",
        "\n",
        "ResNet18 = ResNet(BasicBlock, [2,2,2,2], num_classes = dataset.num_classes)\n",
        "# ResNet34 = ResNet(BasicBlock, [3,4,6,3], num_classes = dataset.num_classes)\n",
        "# ResNet50 = ResNet(BottleNeck, [3,4,6,3], num_classes = dataset.num_classes)\n",
        "# ResNet101 = ResNet(BottleNeck, [3,4,23,3], num_classes = dataset.num_classes)\n",
        "# ResNet152 = ResNet(BottleNeck, [3,8,36,3], num_classes = dataset.num_classes)\n",
        "# se_resnet50 = SEResNet(SEBottleneck, [3, 4, 6, 3], num_classes = dataset.num_classes)\n",
        "# se_resnet101 = SEResNet(SEBottleneck, [3, 4, 23, 3], num_classes = dataset.num_classes)\n",
        "# se_resnet152 = SEResNet(SEBottleneck, [3, 8, 36, 3], num_classes = dataset.num_classes)\n",
        "# se_resnext50 = SEResNeXt(BottleneckX, [3, 4, 6, 3], num_classes = dataset.num_classes)\n",
        "# se_resnext101 = SEResNeXt(BottleneckX, [3, 4, 23, 3], num_classes = dataset.num_classes)\n",
        "# se_resnext152 = SEResNeXt(BottleneckX, [3, 8, 36, 3], num_classes = dataset.num_classes)\n",
        "\n",
        "modelname = \"ResNet18\"\n",
        "model = ResNet18\n",
        "# print(model)\n",
        "\n",
        "if train_on_gpu:\n",
        "  model = torch.nn.DataParallel(model)\n",
        "  cudnn.benchmark = True\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model for basic classification"
      ],
      "metadata": {
        "id": "BQ6JhHmOzjv5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxzRl0QmRvBe",
        "outputId": "8b1e1515-16e4-448e-f68a-e6b70169fb60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.202741 \tValidation Loss: 4.420828\n",
            "Validation loss decreased (inf --> 4.420828).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.901833 \tValidation Loss: 2.798322\n",
            "Validation loss decreased (4.420828 --> 2.798322).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.579068 \tValidation Loss: 0.915781\n",
            "Validation loss decreased (2.798322 --> 0.915781).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.345102 \tValidation Loss: 0.715923\n",
            "Validation loss decreased (0.915781 --> 0.715923).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.216748 \tValidation Loss: 1.362385\n",
            "Epoch: 6 \tTraining Loss: 0.188349 \tValidation Loss: 0.610541\n",
            "Validation loss decreased (0.715923 --> 0.610541).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.225160 \tValidation Loss: 3.497119\n",
            "Epoch: 8 \tTraining Loss: 0.137777 \tValidation Loss: 10.691264\n",
            "Epoch: 9 \tTraining Loss: 0.076822 \tValidation Loss: 1.098655\n",
            "Epoch: 10 \tTraining Loss: 0.053520 \tValidation Loss: 3.752500\n",
            "Epoch: 11 \tTraining Loss: 0.068602 \tValidation Loss: 1.126633\n",
            "Epoch: 12 \tTraining Loss: 0.044876 \tValidation Loss: 0.917548\n",
            "Epoch: 13 \tTraining Loss: 0.060591 \tValidation Loss: 0.854906\n",
            "Epoch: 14 \tTraining Loss: 0.007126 \tValidation Loss: 0.509992\n",
            "Validation loss decreased (0.610541 --> 0.509992).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.008418 \tValidation Loss: 0.700052\n",
            "Epoch: 16 \tTraining Loss: 0.015296 \tValidation Loss: 0.580770\n",
            "Epoch: 17 \tTraining Loss: 0.004857 \tValidation Loss: 0.687281\n",
            "Epoch: 18 \tTraining Loss: 0.001873 \tValidation Loss: 0.623625\n",
            "Epoch: 19 \tTraining Loss: 0.001041 \tValidation Loss: 0.626333\n",
            "Epoch: 20 \tTraining Loss: 0.000707 \tValidation Loss: 0.590274\n"
          ]
        }
      ],
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 20\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  # keep track of training and validation loss\n",
        "  train_loss = 0.0\n",
        "  valid_loss = 0.0\n",
        "    \n",
        "  ###################\n",
        "  # train the model #\n",
        "  ###################\n",
        "  model.train()\n",
        "  for batch_idx, (images, contents, fonts, authors) in enumerate(train_loader):\n",
        "    resize = transforms.Compose([transforms.Resize(224)])\n",
        "    data = resize(images.unsqueeze(1)).float()\n",
        "    target = fonts.long()\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # clear the gradients of all optimized variables\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # backward pass: compute gradient of the loss with respect to model parameters\n",
        "    loss.backward()\n",
        "    # perform a single optimization step (parameter update)\n",
        "    optimizer.step()\n",
        "    # update training loss\n",
        "    train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "  ######################    \n",
        "  # validate the model #\n",
        "  ######################\n",
        "  model.eval()\n",
        "  for batch_idx, (images, contents, fonts, authors) in enumerate(valid_loader):\n",
        "    resize = transforms.Compose([transforms.Resize(224)])\n",
        "    data = resize(images.unsqueeze(1)).float()\n",
        "    target = fonts.long()\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "      data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update average validation loss \n",
        "    valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "  # calculate average losses\n",
        "  train_loss = train_loss/len(train_loader.sampler)\n",
        "  valid_loss = valid_loss/len(valid_loader.sampler)\n",
        "        \n",
        "  # print training/validation statistics \n",
        "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch, train_loss, valid_loss))\n",
        "    \n",
        "  # save model if validation loss has decreased\n",
        "  if valid_loss <= valid_loss_min:\n",
        "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "    valid_loss_min,\n",
        "    valid_loss))\n",
        "    torch.save(model.state_dict(), modelname + '.pt')\n",
        "    valid_loss_min = valid_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test model for basic classification"
      ],
      "metadata": {
        "id": "lIcY86-75xNN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AhGdqGkRvBf",
        "outputId": "8fcbabe8-5d76-4aab-d51f-30fe578b4cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.078204\n",
            "\n",
            "Test Accuracy of ['clerical']: 80% ( 4/ 5)\n",
            "Test Accuracy of ['cursive']: 79% (47/59)\n",
            "Test Accuracy of ['regular']: 88% (56/63)\n",
            "Test Accuracy of ['seal']: 75% ( 9/12)\n",
            "Test Accuracy of ['semi-cursive']: 86% (130/151)\n",
            "\n",
            "Test Accuracy (Overall): 84% (246/290)\n"
          ]
        }
      ],
      "source": [
        "# track test loss\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "# iterate over test data\n",
        "for batch_idx, (images, contents, fonts, authors) in enumerate(test_loader):\n",
        "  resize = transforms.Compose([transforms.Resize(224)])\n",
        "  data = resize(images.unsqueeze(1)).float()\n",
        "  target = fonts.long()\n",
        "  # move tensors to GPU if CUDA is available\n",
        "  if train_on_gpu:\n",
        "    data, target = data.cuda(), target.cuda()\n",
        "  # forward pass: compute predicted outputs by passing inputs to the model\n",
        "  output = model(data)\n",
        "  # calculate the batch loss\n",
        "  loss = criterion(output, target)\n",
        "  # update test loss \n",
        "  test_loss += loss.item()*data.size(0)\n",
        "  # convert output probabilities to predicted class\n",
        "  _, pred = torch.max(output, 1)    \n",
        "  # compare predictions to true label\n",
        "  correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "  correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "  # calculate test accuracy for each object class\n",
        "  for i in range(min(batch_size, len(target.data))):\n",
        "    label = target.data[i]\n",
        "    class_correct[label] += correct[i].item()\n",
        "    class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(dataset.num_classes):\n",
        "  if class_total[i] > 0:\n",
        "    print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "        dataset.fontencoder.inverse_transform([i]), 100 * class_correct[i] / class_total[i],\n",
        "        np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "  else:\n",
        "    print('Test Accuracy of %5s: N/A (no training examples)' % (dataset.fontencoder.inverse_transform([i])))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project_data_loader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}